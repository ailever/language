{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce2e50b-f5df-4c4c-b3e6-9f4f2a0e669e",
   "metadata": {},
   "source": [
    "## Word Sentiment Classification\n",
    "\n",
    "### List of contents\n",
    "1. Data Preprocessing\n",
    "2. Forward Function\n",
    "3. Backward Function\n",
    "4. Training Loop Instances\n",
    "5. Train&Validation Step Function\n",
    "6. Training\n",
    "7. Inference&Visualization\n",
    "Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14fb19c-70a0-40f9-8b98-b481d5741dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing, layers, models, losses, optimizers, metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77c2f4-f048-44d1-aba1-86d2ff4166c1",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba554e65-df76-4c6d-81f8-4b5fe1019257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "train_dataset = {\n",
    "    'Text':   ['good', 'bad', 'worse', 'so good', 'great', 'best', 'false', 'true', 'better', 'mad'],\n",
    "    'Target': [1     , 0    , 0      , 1        , 1      , 1     , 0      , 1     , 1       , 0    ]\n",
    "}\n",
    "validation_dataset  = {\n",
    "    'Text':   ['goodd', 'worrse', 'greattt', 'sad'],\n",
    "    'Target': [1      , 0       , 1        , 0    ]\n",
    "}\n",
    "\n",
    "# Fit Tokenizer with train_dataset\n",
    "sequence_length = 1+max(map(lambda word: len(word) , train_dataset['Text']))\n",
    "num_unique_char = len(set(''.join(train_dataset['Text'])))\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=False,  split='!', char_level=False, oov_token=\"<OOV>\", document_count=0)\n",
    "tokenizer.fit_on_texts(list(''.join(train_dataset['Text'])))  #tokenizer.word_index\n",
    "\n",
    "# Preprocessing\n",
    "train_sequences = tokenizer.texts_to_sequences(list(map(lambda word: list(word), train_dataset['Text'])))\n",
    "validation_sequences = tokenizer.texts_to_sequences(list(map(lambda word: list(word), validation_dataset['Text'])))\n",
    "train_pad_sequences = preprocessing.sequence.pad_sequences(train_sequences, value=0, padding='post', maxlen=sequence_length) # padding: 'pre' or 'post'\n",
    "validation_pad_sequences = preprocessing.sequence.pad_sequences(validation_sequences, value=0, padding='post', maxlen=sequence_length) # padding: 'pre' or 'post'\n",
    "train_onehot_sequences = tf.one_hot(train_pad_sequences, depth=num_unique_char)\n",
    "validation_onehot_sequences = tf.one_hot(validation_pad_sequences, depth=num_unique_char)\n",
    "\n",
    "train_X = train_onehot_sequences\n",
    "train_Y = train_dataset['Target']\n",
    "train_Text = train_dataset['Text']\n",
    "validation_X = validation_onehot_sequences\n",
    "validation_Y = validation_dataset['Target']\n",
    "validation_Text = validation_dataset['Text']\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y))\n",
    "train_dataset = train_dataset.interleave(lambda x, y: tf.data.Dataset.from_tensors((x, y)).repeat(100), cycle_length=4, block_length=7).batch(5)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_X, validation_Y))\n",
    "validation_dataset = validation_dataset.interleave(lambda x, y: tf.data.Dataset.from_tensors((x, y)), cycle_length=4, block_length=7).batch(2)\n",
    "inference_dataset = tf.data.Dataset.from_tensor_slices((validation_X, validation_Y, validation_Text)).batch(batch_size=len(validation_Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1650bf-5ac5-407e-857c-69a9cda8f683",
   "metadata": {},
   "source": [
    "## 2. Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da34a144-140d-43ae-9592-9a7408df2581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(models.Model):\n",
    "    def __init__(self, name=None):\n",
    "        super(Model, self).__init__(name=name)\n",
    "        self.dense_1 = layers.Dense(5, activation='relu')\n",
    "        self.rnn = layers.SimpleRNN(units=32)\n",
    "        self.dense_2 = layers.Dense(units=1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense_2(x)\n",
    "        return tf.squeeze(x, axis=None)\n",
    "    \n",
    "class Criterion(losses.Loss):\n",
    "    def call(self, target, hypothesis, training=None):\n",
    "        return tf.reduce_mean(losses.binary_crossentropy(y_true=target, y_pred=hypothesis, from_logits=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233620e-35fb-4d91-b3ed-43ba3298cfb6",
   "metadata": {},
   "source": [
    "## 3. Backward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aaf93f7-a269-4c0c-a428-511b9b7da3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "\n",
    "    def __call__(self, step):\n",
    "        return self.initial_learning_rate / (step + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa604cc-b738-44e7-bc52-632bc69a7fd7",
   "metadata": {},
   "source": [
    "## 4. Training Loop Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18645730-8f25-4d4a-bdc7-1b9943af9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(name='Model')\n",
    "criterion = Criterion()\n",
    "lr_schedule = LearningRateScheduler(initial_learning_rate=0.1)\n",
    "optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_accuracy = metrics.BinaryAccuracy(name='train_accuracy')\n",
    "validation_loss = metrics.Mean(name='validation_loss')\n",
    "validation_accuracy = metrics.BinaryAccuracy(name='validation_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6685c0a-e2fe-45c2-aa7e-deaad8e9e29c",
   "metadata": {},
   "source": [
    "## 5. Train&Validation Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aded0f82-fd68-4b8e-bda4-cb2fb81b8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Train Step]\n",
    "@tf.function\n",
    "def train_step(features, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = model(features, training=True)\n",
    "        cost = criterion(targets, hypothesis)\n",
    "    gradients = tape.gradient(cost, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(cost)\n",
    "    train_accuracy.update_state(targets, hypothesis)\n",
    "    \n",
    "# [Validation Step]\n",
    "@tf.function\n",
    "def validation_step(features, targets):\n",
    "    predictions = model(features, training=False)\n",
    "    cost = criterion(targets, predictions)\n",
    "\n",
    "    validation_loss.update_state(cost)\n",
    "    validation_accuracy.update_state(targets, predictions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524f646-f87c-4112-a682-48c87ac51922",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d02a713-1b91-4ef2-bde6-69a1671b5afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.29046374559402466, Accuracy: 90.39999389648438, Validation Loss: 0.10069369524717331, Validation Accuracy: 100.0\n",
      "Epoch 2, Loss: 0.06729231774806976, Accuracy: 100.0, Validation Loss: 0.06611797213554382, Validation Accuracy: 100.0\n",
      "Epoch 3, Loss: 0.050355296581983566, Accuracy: 100.0, Validation Loss: 0.05259763449430466, Validation Accuracy: 100.0\n",
      "Epoch 4, Loss: 0.0418490469455719, Accuracy: 100.0, Validation Loss: 0.044528745114803314, Validation Accuracy: 100.0\n",
      "Epoch 5, Loss: 0.03638181462883949, Accuracy: 100.0, Validation Loss: 0.03906872868537903, Validation Accuracy: 100.0\n",
      "Epoch 6, Loss: 0.03246432915329933, Accuracy: 100.0, Validation Loss: 0.03503796458244324, Validation Accuracy: 100.0\n",
      "Epoch 7, Loss: 0.02945704199373722, Accuracy: 100.0, Validation Loss: 0.03188469260931015, Validation Accuracy: 100.0\n",
      "Epoch 8, Loss: 0.027039000764489174, Accuracy: 100.0, Validation Loss: 0.029317840933799744, Validation Accuracy: 100.0\n",
      "Epoch 9, Loss: 0.02502952143549919, Accuracy: 100.0, Validation Loss: 0.02716604806482792, Validation Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.023317977786064148, Accuracy: 100.0, Validation Loss: 0.025322310626506805, Validation Accuracy: 100.0\n",
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  80        \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      multiple                  1216      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329\n",
      "Trainable params: 1,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    validation_accuracy.reset_states()\n",
    "\n",
    "    # Training Step\n",
    "    for features, targets in train_dataset:\n",
    "        train_step(features, targets)\n",
    "\n",
    "    # Validation Step\n",
    "    for features, targets in validation_dataset:\n",
    "        validation_step(features, targets)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Validation Loss: {validation_loss.result()}, '\n",
    "        f'Validation Accuracy: {validation_accuracy.result() * 100}'\n",
    "    )\n",
    "\n",
    "model.save_weights('model.ckpt')\n",
    "model.load_weights('model.ckpt')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1774a7-3b42-4a6f-991f-6a92235e5cc1",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f145156-8cc8-472f-b272-1caa1772ad08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'goodd'</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'worrse'</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'greattt'</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'sad'</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Input Sentiment\n",
       "0    b'goodd'       1.0\n",
       "1   b'worrse'       0.0\n",
       "2  b'greattt'       1.0\n",
       "3      b'sad'       0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction = model(list(inference_dataset)[0][0]).numpy().round(0)\n",
    "prediction = model.predict(inference_dataset).round(0)\n",
    "inference = pd.DataFrame(np.c_[list(inference_dataset)[0][2].numpy(), prediction], columns=['Input', 'Sentiment'])\n",
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ed67f-44c9-4a01-8802-08be8d6fa7f7",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c4f69-e17a-411c-8639-64b9f558ff5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
